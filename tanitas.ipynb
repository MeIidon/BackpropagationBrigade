{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heSLeiKApG55"
      },
      "outputs": [],
      "source": [
        "# Installing kaggle to download the dataset.\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need kaggle credentials which needs to be stored in ~/.kaggle/kaggle.json\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "tlCfhA-_s-73"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Reading in the contents of user's kaggle.json.\n",
        "kaggle_secret = getpass('Enter the content of your kaggle.json: ')\n",
        "\n",
        "# Saving into the previously created file.\n",
        "with open('/root/.kaggle/kaggle.json', \"w\") as f:\n",
        "  f.write(kaggle_secret)"
      ],
      "metadata": {
        "id": "XPP0_iXQqngd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and unzipping the training dataset.\n",
        "!kaggle competitions download -c tpu-getting-started\n",
        "!unzip tpu-getting-started.zip -d data\n",
        "!rm tpu-getting-started.zip"
      ],
      "metadata": {
        "id": "Nt5GbDF5sHy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Paths to the training, validation and test datasets.\n",
        "DATASET_PATH = \"data\"\n",
        "PATH = DATASET_PATH + \"/tfrecords-jpeg-512x512\"\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(PATH + \"/train/*.tfrec\")\n",
        "VALIDATION_FILENAMES = tf.io.gfile.glob(PATH + \"/val/*.tfrec\")\n",
        "TEST_FILENAMES = tf.io.gfile.glob(PATH + \"/test/*.tfrec\")\n",
        "\n",
        "# Training related constants.\n",
        "IMAGE_SIZE = [512, 512]\n",
        "BATCH_SIZE = 32\n",
        "NUM_OF_CLASSES = 104"
      ],
      "metadata": {
        "id": "hwHaiyUWt_mz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts the raw image data to a [IMAGE_SIZE, 3] shaped array\n",
        "# containing the normalized color intensity values for all channels.\n",
        "def decode_image(image_data):\n",
        "  # Extracting the image from the dataset.\n",
        "  image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "  # Normalizing the color intensity values.\n",
        "  image = (\n",
        "      tf.cast(image, tf.float32) / 255.0\n",
        "  )\n",
        "  # Reshaping for 3 channels.\n",
        "  image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "  return image\n",
        "\n",
        "\n",
        "# This function takes a raw labelled tfrecord and converts it to decoded image data\n",
        "# and one hot encoded label.\n",
        "def read_labelled_tfrecord(example):\n",
        "  LABELLED_TFREC_FORMAT = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"class\": tf.io.FixedLenFeature([], tf.int64),\n",
        "  }\n",
        "  # Converting the raw data to a python dictionary with LABELLED_TFREC_FORMAT.\n",
        "  example = tf.io.parse_single_example(example, LABELLED_TFREC_FORMAT)\n",
        "  # Decoding the image.\n",
        "  image = decode_image(example[\"image\"])\n",
        "  # One-hot encode the label of the image.\n",
        "  label = tf.cast(example[\"class\"], tf.int32)\n",
        "  one_hot = tf.one_hot(label, NUM_OF_CLASSES)\n",
        "  return image, one_hot\n",
        "\n",
        "\n",
        "# This function takes a raw unlabelled tfrecord and converts it to decoded image data\n",
        "# and the id of the test image.\n",
        "def read_unlabelled_tfrecord(example):\n",
        "  UNLABELLED_TFREC_FORMAT = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  # Converting raw data to a python dictionary with UNLABELLED_TFREC_FORMAT.\n",
        "  example = tf.io.parse_single_example(example, UNLABELLED_TFREC_FORMAT)\n",
        "  # Decoding the image.\n",
        "  image = decode_image(example[\"image\"])\n",
        "  id_num = example[\"id\"]\n",
        "  return image, id_num\n",
        "\n",
        "\n",
        "# This function takes several filenames (tfrecords) and creates a tensorflow\n",
        "# dataset from them.\n",
        "def load_dataset(filenames, labelled=True, ordered=False):\n",
        "  # The order of the images doesn't matter so we are turning on ignore_order\n",
        "  # this results in faster loading times.\n",
        "  ignore_order = tf.data.Options()\n",
        "  if not ordered:\n",
        "    ignore_order.experimental_deterministic = False\n",
        "\n",
        "  # Creating the dataset. We are using parallel read option because obviously\n",
        "  # it results in faster loading times.\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "    filenames, num_parallel_reads=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  dataset = dataset.with_options(ignore_order)\n",
        "  # The final element of the pipeline is taking the raw tfrecord\n",
        "  # and converting it to labelled on unlabelled input data with the upper functions.\n",
        "  dataset = dataset.map(\n",
        "    read_labelled_tfrecord if labelled else read_unlabelled_tfrecord,\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "  )\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "DI-dXAxuxm9o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function creates the training dataset.\n",
        "# repeat will be used, this will make sure that after reaching the end record\n",
        "# we go back to the first.\n",
        "# BATCH_SIZE will be used.\n",
        "# We will prefetch 1 batch for accellerating the reading process.\n",
        "def get_training_dataset():\n",
        "  dataset = load_dataset(TRAINING_FILENAMES, labelled=True)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "# This function creates the validation dataset.\n",
        "# BATCH_SIZE will be used.\n",
        "# We will prefetch 1 batch for accellerating the reading process.\n",
        "def get_validation_dataset(ordered=False):\n",
        "  dataset = load_dataset(VALIDATION_FILENAMES, labelled=True, ordered=ordered)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# This function creates the test dataset.\n",
        "# BATCH_SIZE will be used.\n",
        "# We will prefetch 1 batch for accellerating the reading process.\n",
        "def get_test_dataset(ordered=False):\n",
        "  dataset = load_dataset(TEST_FILENAMES, labelled=False, ordered=ordered)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# This function counts the number of tfrecords in a dataset.\n",
        "def count_data_items(filenames):\n",
        "  n = [\n",
        "    int(re.compile(r\"-(\\d*)\\.\").search(filename).group(1))\n",
        "    for filename in filenames\n",
        "  ]\n",
        "  return np.sum(n)"
      ],
      "metadata": {
        "id": "boA0dEsByty-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be used to calculate the number of batches needed to go through\n",
        "# all the dataset in an epoch.\n",
        "num_train_images = count_data_items(TRAINING_FILENAMES)\n",
        "\n",
        "# Getting the train, validation and test datasets.\n",
        "train_dataset = get_training_dataset()\n",
        "validation_dataset = get_validation_dataset()\n",
        "test_dataset = get_test_dataset()"
      ],
      "metadata": {
        "id": "UQFiJq8dzhZn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For hyperparameter-optimization keras-tuner will be used.\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "yNIpfTEghpVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets, layers, models, callbacks, applications, optimizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "  # Using Transfer Learning technique for this project.\n",
        "  # InceptionV3 pretrained model will be used.\n",
        "  pre_trained_model = applications.InceptionV3(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "  # Freezing the weights of the model.\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Adding our own model on top of InceptionV3.\n",
        "  x = pre_trained_model.output\n",
        "  # Using pooling to decrease dimension.\n",
        "  x = layers.GlobalAveragePooling2D()(x)\n",
        "  # Putting some hidden layers depending on the current\n",
        "  # hyperparameter-optimization iteration.\n",
        "  for i in range(hp.Int('num_layers', min_value=1, max_value=3)):\n",
        "    # A hidden layer with optimized size and activation function.\n",
        "    x = layers.Dense(\n",
        "        hp.Int('units_' + str(i), 256, 512, 32),\n",
        "        activation='relu')(x)\n",
        "    # Putting dropout after every hidden layer.\n",
        "    x = layers.Dropout(hp.Choice('dropout_' + str(i), [0.3, 0.4, 0.5]))(x)\n",
        "  # The output will be a probability distribution of classes.\n",
        "  x = layers.Dense(NUM_OF_CLASSES, activation='softmax')(x)\n",
        "\n",
        "  # Creating and compiling the final model from the pretrained model and our own model.\n",
        "  model = models.Model(pre_trained_model.input, x)\n",
        "  optimizer = optimizers.Adam(\n",
        "      learning_rate=hp.Choice('learning_rate', [0.01, 0.001, 0.0001])\n",
        "  )\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=['acc']\n",
        "  )\n",
        "  return model\n",
        "\n",
        "# Using keras-tuner for hyperparameter-optimization.\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_acc',\n",
        "    max_epochs=5,\n",
        "    factor=3,\n",
        "    directory='best_model',\n",
        "    project_name='FlowerClassificationModel'\n",
        ")\n",
        "\n",
        "# Using EarlyStopping to prevent overfitting.\n",
        "es = callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='max')\n",
        "\n",
        "# Searching with Hyperband in the hyperparameter-space.\n",
        "tuner.search(train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=25,\n",
        "    steps_per_epoch=num_train_images / BATCH_SIZE,\n",
        "    callbacks=[es])"
      ],
      "metadata": {
        "id": "c4lKwPTy1M8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Saving the best model so far.\n",
        "cp = callbacks.ModelCheckpoint(filepath='model', monitor='val_acc', mode='max')\n",
        "# Tensorboard for visualizing.\n",
        "tb = callbacks.TensorBoard(update_freq=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=num_train_images / BATCH_SIZE,\n",
        "    callbacks=[es, cp, tb])"
      ],
      "metadata": {
        "id": "mfGAD4qBtav3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}