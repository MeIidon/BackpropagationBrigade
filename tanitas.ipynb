{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "heSLeiKApG55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35843e3-968e-4599-c1b7-f38b22d4a308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "# Installing kaggle to download the dataset.\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need kaggle credentials which needs to be stored in ~/.kaggle/kaggle.json\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "tlCfhA-_s-73"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Reading in the contents of user's kaggle.json.\n",
        "kaggle_secret = getpass('Enter the content of your kaggle.json: ')\n",
        "\n",
        "# Saving into the previously created file.\n",
        "with open('/root/.kaggle/kaggle.json', \"w\") as f:\n",
        "  f.write(kaggle_secret)"
      ],
      "metadata": {
        "id": "XPP0_iXQqngd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd275aa-f7f9-4390-cc3d-40ce6acd0381"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the content of your kaggle.json: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and unzipping the training dataset.\n",
        "!kaggle competitions download -c tpu-getting-started\n",
        "!unzip tpu-getting-started.zip -d data\n",
        "!rm tpu-getting-started.zip"
      ],
      "metadata": {
        "id": "Nt5GbDF5sHy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fdb71c-aa18-452b-e7e7-82cccb15f24d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading tpu-getting-started.zip to /content\n",
            "100% 4.79G/4.79G [02:41<00:00, 37.2MB/s]\n",
            "100% 4.79G/4.79G [02:41<00:00, 31.8MB/s]\n",
            "Archive:  tpu-getting-started.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/00-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/01-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/02-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/03-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/04-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/05-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/06-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/07-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/08-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/09-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/10-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/11-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/12-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/13-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/14-192x192-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/test/15-192x192-452.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/00-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/01-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/02-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/03-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/04-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/05-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/06-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/07-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/08-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/09-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/10-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/11-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/12-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/13-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/14-192x192-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/train/15-192x192-783.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/00-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/01-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/02-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/03-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/04-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/05-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/06-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/07-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/08-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/09-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/10-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/11-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/12-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/13-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/14-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-192x192/val/15-192x192-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/00-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/01-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/02-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/03-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/04-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/05-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/06-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/07-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/08-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/09-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/10-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/11-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/12-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/13-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/14-224x224-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/test/15-224x224-452.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/00-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/01-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/02-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/03-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/04-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/05-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/06-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/07-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/08-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/09-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/10-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/11-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/12-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/13-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/14-224x224-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/train/15-224x224-783.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/00-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/01-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/02-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/03-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/04-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/05-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/06-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/07-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/08-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/09-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/10-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/11-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/12-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/13-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/14-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-224x224/val/15-224x224-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/00-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/01-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/02-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/03-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/04-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/05-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/06-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/07-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/08-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/09-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/10-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/11-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/12-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/13-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/14-331x331-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/test/15-331x331-452.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/00-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/01-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/02-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/03-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/04-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/05-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/06-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/07-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/08-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/09-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/10-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/11-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/12-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/13-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/14-331x331-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/train/15-331x331-783.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/00-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/01-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/02-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/03-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/04-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/05-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/06-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/07-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/08-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/09-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/10-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/11-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/12-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/13-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/14-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-331x331/val/15-331x331-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/00-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/01-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/02-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/03-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/04-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/05-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/06-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/07-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/08-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/09-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/10-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/11-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/12-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/13-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/14-512x512-462.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/test/15-512x512-452.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/00-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/01-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/02-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/03-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/04-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/05-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/06-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/07-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/08-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/09-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/10-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/11-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/12-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/13-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/14-512x512-798.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/train/15-512x512-783.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/00-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/01-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/02-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/03-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/04-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/05-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/06-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/07-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/08-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/09-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/10-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/11-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/12-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/13-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/14-512x512-232.tfrec  \n",
            "  inflating: data/tfrecords-jpeg-512x512/val/15-512x512-232.tfrec  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Paths to the training, validation and test datasets.\n",
        "DATASET_PATH = \"data\"\n",
        "PATH = DATASET_PATH + \"/tfrecords-jpeg-224x224\"\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(PATH + \"/train/*.tfrec\")\n",
        "VALIDATION_FILENAMES = tf.io.gfile.glob(PATH + \"/val/*.tfrec\")\n",
        "TEST_FILENAMES = tf.io.gfile.glob(PATH + \"/test/*.tfrec\")\n",
        "\n",
        "# Training related constants.\n",
        "IMAGE_SIZE = [224, 224]\n",
        "BATCH_SIZE = 32\n",
        "NUM_OF_CLASSES = 104"
      ],
      "metadata": {
        "id": "hwHaiyUWt_mz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts the raw image data to a [IMAGE_SIZE, 3] shaped array\n",
        "# containing the normalized color intensity values for all channels.\n",
        "def decode_image(image_data):\n",
        "  # Extracting the image from the dataset.\n",
        "  image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "  # Normalizing the color intensity values.\n",
        "  image = (\n",
        "      tf.cast(image, tf.float32) / 255.0\n",
        "  )\n",
        "  # Reshaping for 3 channels.\n",
        "  image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "  return image\n",
        "\n",
        "\n",
        "# This function takes a raw labelled tfrecord and converts it to decoded image data\n",
        "# and one hot encoded label.\n",
        "def read_labelled_tfrecord(example):\n",
        "  LABELLED_TFREC_FORMAT = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"class\": tf.io.FixedLenFeature([], tf.int64),\n",
        "  }\n",
        "  # Converting the raw data to a python dictionary with LABELLED_TFREC_FORMAT.\n",
        "  example = tf.io.parse_single_example(example, LABELLED_TFREC_FORMAT)\n",
        "  # Decoding the image.\n",
        "  image = decode_image(example[\"image\"])\n",
        "  # One-hot encode the label of the image.\n",
        "  label = tf.cast(example[\"class\"], tf.int32)\n",
        "  one_hot = tf.one_hot(label, NUM_OF_CLASSES)\n",
        "  return image, one_hot\n",
        "\n",
        "\n",
        "# This function takes a raw unlabelled tfrecord and converts it to decoded image data\n",
        "# and the id of the test image.\n",
        "def read_unlabelled_tfrecord(example):\n",
        "  UNLABELLED_TFREC_FORMAT = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  # Converting raw data to a python dictionary with UNLABELLED_TFREC_FORMAT.\n",
        "  example = tf.io.parse_single_example(example, UNLABELLED_TFREC_FORMAT)\n",
        "  # Decoding the image.\n",
        "  image = decode_image(example[\"image\"])\n",
        "  id_num = example[\"id\"]\n",
        "  return image, id_num\n",
        "\n",
        "\n",
        "# This function takes several filenames (tfrecords) and creates a tensorflow\n",
        "# dataset from them.\n",
        "def load_dataset(filenames, labelled=True, ordered=False):\n",
        "  # The order of the images doesn't matter so we are turning on ignore_order\n",
        "  # this results in faster loading times.\n",
        "  ignore_order = tf.data.Options()\n",
        "  if not ordered:\n",
        "    ignore_order.experimental_deterministic = False\n",
        "\n",
        "  # Creating the dataset. We are using parallel read option because obviously\n",
        "  # it results in faster loading times.\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "    filenames, num_parallel_reads=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  dataset = dataset.with_options(ignore_order)\n",
        "  # The final element of the pipeline is taking the raw tfrecord\n",
        "  # and converting it to labelled on unlabelled input data with the upper functions.\n",
        "  dataset = dataset.map(\n",
        "    read_labelled_tfrecord if labelled else read_unlabelled_tfrecord,\n",
        "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "  )\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "DI-dXAxuxm9o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function creates the training dataset.\n",
        "# repeat will be used, this will make sure that after reaching the end record\n",
        "# we go back to the first.\n",
        "# BATCH_SIZE will be used.\n",
        "# We will prefetch 1 batch for accellerating the reading process.\n",
        "def get_training_dataset():\n",
        "  dataset = load_dataset(TRAINING_FILENAMES, labelled=True)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "# This function creates the validation dataset.\n",
        "# BATCH_SIZE will be used.\n",
        "# We will prefetch 1 batch for accellerating the reading process.\n",
        "def get_validation_dataset(ordered=False):\n",
        "  dataset = load_dataset(VALIDATION_FILENAMES, labelled=True, ordered=ordered)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# This function creates the test dataset.\n",
        "# BATCH_SIZE will be used.\n",
        "# We will prefetch 1 batch for accellerating the reading process.\n",
        "def get_test_dataset(ordered=False):\n",
        "  dataset = load_dataset(TEST_FILENAMES, labelled=False, ordered=ordered)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# This function counts the number of tfrecords in a dataset.\n",
        "def count_data_items(filenames):\n",
        "  n = [\n",
        "    int(re.compile(r\"-(\\d*)\\.\").search(filename).group(1))\n",
        "    for filename in filenames\n",
        "  ]\n",
        "  return np.sum(n)"
      ],
      "metadata": {
        "id": "boA0dEsByty-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be used to calculate the number of batches needed to go through\n",
        "# all the dataset in an epoch.\n",
        "num_train_images = count_data_items(TRAINING_FILENAMES)\n",
        "\n",
        "# Getting the train, validation and test datasets.\n",
        "train_dataset = get_training_dataset()\n",
        "validation_dataset = get_validation_dataset()\n",
        "test_dataset = get_test_dataset()"
      ],
      "metadata": {
        "id": "UQFiJq8dzhZn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For hyperparameter-optimization keras-tuner will be used.\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "yNIpfTEghpVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b8bdd7-a8e6-4e39-c34d-e7a6ddd18d2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets, layers, models, callbacks, applications, optimizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "  # Using Transfer Learning technique for this project.\n",
        "  # InceptionV3 pretrained model will be used.\n",
        "  pre_trained_model = applications.InceptionV3(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "  # Freezing the weights of the model.\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Adding our own model on top of InceptionV3.\n",
        "  x = pre_trained_model.output\n",
        "  # Using pooling to decrease dimension.\n",
        "  x = layers.GlobalAveragePooling2D()(x)\n",
        "  # Putting one hidden layer with units.\n",
        "  x = layers.Dense(hp.Choice('units', [256, 512]), activation='relu')(x)\n",
        "  # The output will be a probability distribution of classes.\n",
        "  x = layers.Dense(NUM_OF_CLASSES, activation='softmax')(x)\n",
        "\n",
        "  # Creating and compiling the final model from the pretrained model and our own model.\n",
        "  model = models.Model(pre_trained_model.input, x)\n",
        "  optimizer = optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=['acc']\n",
        "  )\n",
        "  return model\n",
        "\n",
        "# Using keras-tuner for hyperparameter-optimization.\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_acc',\n",
        "    max_epochs=5,\n",
        "    factor=3,\n",
        "    directory='best_model',\n",
        "    project_name='FlowerClassificationModel',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Using EarlyStopping to prevent overfitting.\n",
        "es = callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='max')\n",
        "\n",
        "# Searching with Hyperband in the hyperparameter-space.\n",
        "tuner.search(train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=25,\n",
        "    steps_per_epoch=num_train_images / BATCH_SIZE,\n",
        "    callbacks=[es])"
      ],
      "metadata": {
        "id": "c4lKwPTy1M8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336f3f35-6913-4164-e531-f68b8b5b12f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 01m 31s]\n",
            "val_acc: 0.6608297228813171\n",
            "\n",
            "Best val_acc So Far: 0.6737607717514038\n",
            "Total elapsed time: 00h 04m 02s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the model with the optimal hyperparameters and train it on the data for 10 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Saving the best model so far.\n",
        "cp = callbacks.ModelCheckpoint(filepath='model', monitor='val_acc', mode='max')\n",
        "# Tensorboard for visualizing.\n",
        "tb = callbacks.TensorBoard(update_freq=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=num_train_images / BATCH_SIZE,\n",
        "    callbacks=[es, cp, tb])"
      ],
      "metadata": {
        "id": "mfGAD4qBtav3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59714edc-f891-48f0-b9f6-62c16af5698e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "398/398 [==============================] - 75s 167ms/step - loss: 2.1830 - acc: 0.4848 - val_loss: 1.4900 - val_acc: 0.6166\n",
            "Epoch 2/10\n",
            "398/398 [==============================] - 68s 171ms/step - loss: 1.1542 - acc: 0.6904 - val_loss: 1.2703 - val_acc: 0.6692\n",
            "Epoch 3/10\n",
            "398/398 [==============================] - 71s 177ms/step - loss: 0.8282 - acc: 0.7717 - val_loss: 1.2263 - val_acc: 0.6840\n",
            "Epoch 4/10\n",
            "398/398 [==============================] - 68s 171ms/step - loss: 0.6170 - acc: 0.8331 - val_loss: 1.2471 - val_acc: 0.6786\n",
            "Epoch 5/10\n",
            "398/398 [==============================] - 70s 177ms/step - loss: 0.4735 - acc: 0.8730 - val_loss: 1.2914 - val_acc: 0.6794\n",
            "Epoch 6/10\n",
            "398/398 [==============================] - 69s 173ms/step - loss: 0.3651 - acc: 0.9039 - val_loss: 1.2879 - val_acc: 0.6880\n",
            "Epoch 7/10\n",
            "398/398 [==============================] - 68s 171ms/step - loss: 0.2957 - acc: 0.9214 - val_loss: 1.3378 - val_acc: 0.6913\n",
            "Epoch 8/10\n",
            "398/398 [==============================] - 70s 176ms/step - loss: 0.2450 - acc: 0.9335 - val_loss: 1.3527 - val_acc: 0.6972\n",
            "Epoch 9/10\n",
            "398/398 [==============================] - 67s 168ms/step - loss: 0.2182 - acc: 0.9371 - val_loss: 1.5454 - val_acc: 0.6713\n",
            "Epoch 10/10\n",
            "398/398 [==============================] - 66s 166ms/step - loss: 0.1792 - acc: 0.9474 - val_loss: 1.6144 - val_acc: 0.6813\n"
          ]
        }
      ]
    }
  ]
}